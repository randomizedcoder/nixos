# vLLM Setup Design Document

## System Overview

**Machine:** `l` (desktop)
- **CPU:** AMD Ryzen Threadripper PRO 3945WX (12-cores, Zen 2 architecture)
  - Supports: AVX, AVX2, SSE4.1, SSE4.2
  - Does NOT support: AVX-512
- **Primary GPU:** AMD MI50 (gfx906) - 32GB VRAM - UUID: `GPU-0e54792172da5eeb`
- **Secondary GPU:** AMD Radeon Pro W7500 (gfx1102)
- **OS:** NixOS (unstable channel)

## Current State

### Working: Ollama with ROCm
The existing `ollama-service.nix` successfully uses the MI50 GPU via:
```nix
package = pkgs.ollama-rocm;
environmentVariables = {
  HIP_VISIBLE_DEVICES = "GPU-0e54792172da5eeb";
};
```

### Broken: vLLM
```bash
$ vllm --version
INFO [importing.py:44] Triton is installed but 0 active driver(s) found (expected 1).
                       Disabling Triton to prevent runtime errors.
INFO [importing.py:68] Triton not installed or not compatible...
Illegal instruction (core dumped)
```

**Root Cause Analysis:**
1. The installed vllm package (`python3.12-vllm-0.13.0`) was built with `VLLM_TARGET_DEVICE=cpu`
2. The CPU backend was likely compiled with AVX-512 instructions
3. Your Threadripper PRO 3945WX (Zen 2) does NOT support AVX-512, only AVX2
4. This causes a SIGILL (Illegal instruction) when attempting to run

**Evidence from derivation:**
```json
{
  "rocmSupport": null,
  "cudaSupport": null,
  "VLLM_TARGET_DEVICE": "cpu"
}
```

## Objectives

### A. Get vLLM Working with AMD GPU (Primary Goal)

**Requirements:**
- Build vllm with ROCm support instead of CPU backend
- Configure for MI50 GPU (gfx906 architecture)
- Setup OpenAI-compatible API server for roo code integration
- Match similar configuration pattern as working ollama setup

### B. Fix NixOS Package for This System

**Requirements:**
- Create an overlay or package override to build vllm with `rocmSupport = true`
- Ensure correct GPU architecture targets (gfx906 for MI50)
- Verify torch is also built with ROCm support

### C. Fix --version Support

**Current behavior:** Crashes before showing version
**Root cause:** The crash happens during Python import, before argument parsing

**After fixing A/B, evaluate:**
- If version still doesn't display properly, investigate `setuptools-scm` version detection
- vllm uses `importlib.metadata.version("vllm")` for --version
- The `_version.py` is generated by setuptools-scm from git tags

---

## Implementation Plan

### Phase 1: Build vLLM with ROCm Support

#### Option 1: Flake Overlay with ROCm (Recommended)

Both torch and vllm support `rocmSupport` flag, which reads from `config.rocmSupport`.
The torch package definition shows:
- `rocmSupport ? config.rocmSupport` (line 97)
- `gpuTargets ? [ ]` (line 99)
- Depends on `rocmPackages` when ROCm enabled

**Approach A: Global ROCm Config**

In your `flake.nix`, enable ROCm globally:

```nix
{
  nixpkgs.config = {
    allowUnfree = true;
    rocmSupport = true;  # Enable ROCm for all packages that support it
  };
}
```

This will cause torch and vllm to be built with ROCm support.

**Approach B: Overlay for vllm-rocm only (More Targeted)**

Create an overlay to build ROCm-enabled versions without affecting other packages:

```nix
# In flake.nix or a separate overlay file
let
  vllm-rocm-overlay = final: prev: {
    python3Packages = prev.python3Packages.override {
      overrides = pyFinal: pyPrev: {
        # ROCm-enabled torch
        torch = pyPrev.torch.override {
          rocmSupport = true;
          cudaSupport = false;
          gpuTargets = [ "gfx906" ];  # MI50
        };
        # ROCm-enabled vllm (inherits torch from above)
        vllm = pyPrev.vllm.override {
          rocmSupport = true;
          cudaSupport = false;
          gpuTargets = [ "gfx906" ];
        };
      };
    };
    # Create a top-level alias
    vllm-rocm = final.python3Packages.vllm;
  };
in
# Apply overlay in your flake
{
  nixpkgs.overlays = [ vllm-rocm-overlay ];
}
```

**Note:** Building torch with ROCm will take significant time (hours) and requires ~50GB disk space.

**Approach C: Use `pkgs.config.rocmSupport` in home.nix**

Replace in `home.nix`:
```nix
# OLD
vllm

# NEW - override inline
(pkgs.python3Packages.vllm.override {
  rocmSupport = true;
  cudaSupport = false;
  gpuTargets = [ "gfx906" ];
})
```

This requires torch to also be ROCm-enabled, which may need overlay.

#### Option 2: Use Docker/Podman Container (Quick Start)

vLLM provides official ROCm docker images. This is the fastest way to test:

```bash
# Test vllm with MI50 GPU
docker run --rm -it \
  --device=/dev/kfd \
  --device=/dev/dri \
  --group-add video \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  -p 8000:8000 \
  --ipc=host \
  vllm/vllm-openai-rocm:latest \
  --model Qwen/Qwen2.5-Coder-7B-Instruct \
  --host 0.0.0.0

# Specify MI50 GPU
HIP_VISIBLE_DEVICES=GPU-0e54792172da5eeb docker run --rm -it \
  --device=/dev/kfd \
  --device=/dev/dri \
  --group-add video \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  -p 8000:8000 \
  --ipc=host \
  vllm/vllm-openai-rocm:latest \
  --model Qwen/Qwen2.5-Coder-7B-Instruct \
  --host 0.0.0.0
```

**Pros:**
- Works immediately, no build time
- Official image tested for ROCm

**Cons:**
- Large image download (~15GB)
- Not integrated with NixOS service management
- Separate Python environment from system

**Recommended: Use Docker for initial testing, then move to native NixOS package if successful.**

### Phase 2: Service Configuration

Create `vllm-service.nix` (similar pattern to ollama-service.nix):

```nix
# vllm-service.nix
{ config, lib, pkgs, ... }:

let
  mi50euid = "GPU-0e54792172da5eeb";
in
{
  # Option A: Systemd service for native package
  systemd.services.vllm = {
    description = "vLLM OpenAI-compatible API Server";
    after = [ "network.target" ];
    wantedBy = [ "multi-user.target" ];

    environment = {
      HIP_VISIBLE_DEVICES = mi50euid;
      # or ROCR_VISIBLE_DEVICES
    };

    serviceConfig = {
      ExecStart = "${vllm-rocm}/bin/vllm serve <model> --host 0.0.0.0 --port 8000";
      Restart = "on-failure";
      # ... other service options
    };
  };

  # Option B: OCI container service
  virtualisation.oci-containers.containers.vllm = {
    image = "vllm/vllm-openai-rocm:latest";
    ports = [ "8000:8000" ];
    extraOptions = [
      "--device=/dev/kfd"
      "--device=/dev/dri"
      "--group-add=video"
    ];
    volumes = [
      "/home/das/.cache/huggingface:/root/.cache/huggingface"
    ];
    cmd = [ "--model" "Qwen/Qwen2.5-Coder-32B-Instruct" "--host" "0.0.0.0" ];
  };
}
```

### Phase 3: OpenAI API Integration

vLLM's serve command provides OpenAI-compatible endpoints:

```bash
# Start server
vllm serve Qwen/Qwen2.5-Coder-32B-Instruct --host 0.0.0.0 --port 8000

# API endpoints available:
# POST /v1/completions
# POST /v1/chat/completions
# GET  /v1/models
```

**For roo code integration:**
- Set API base URL: `http://localhost:8000/v1`
- No API key required for local server (or set `--api-key` on server)

### Phase 4: Version Fix Investigation

**Docker test shows version works:** vLLM 0.15.0 displayed correctly via server banner.

**Version source analysis:**
- Server banner uses: `from vllm.version import __version__ as VLLM_VERSION` (entrypoints/openai/api_server.py:57)
- CLI `--version` uses: `importlib.metadata.version("vllm")` (entrypoints/cli/main.py:59)
- Both read from the same source: package metadata set by setuptools-scm

**Root cause of CLI crash:**
The crash happens BEFORE `--version` is processed. In `main.py`:
```python
# Line 11-13 - crashes HERE during import
from vllm.logger import init_logger
logger = init_logger(__name__)

# Line 55-60 - never reached
parser.add_argument("--version", ..., version=importlib.metadata.version("vllm"))
```

The `vllm.logger` import triggers the triton check which loads compiled code with illegal instructions.

**Potential PR fix for upstream vllm:**
Handle `--version` BEFORE importing any vllm modules:

```python
# At top of main.py, before any vllm imports
import sys
if len(sys.argv) == 2 and sys.argv[1] in ("-v", "--version"):
    import importlib.metadata
    print(importlib.metadata.version("vllm"))
    sys.exit(0)

# Then continue with normal imports
from vllm.logger import init_logger
...
```

This would allow `vllm --version` to work even when the compiled backend is incompatible.

---

## Build Time Expectations

Building ROCm-enabled packages from source is time-intensive:

| Package | Estimated Build Time | Disk Space |
|---------|---------------------|------------|
| torch (ROCm) | 2-6 hours | ~50GB |
| vllm (ROCm) | 30-60 minutes | ~10GB |
| Total | 3-7 hours | ~60GB |

**Recommendations:**
1. Start with Docker approach to validate vLLM works with your GPU
2. If Docker works, proceed with native NixOS build
3. Consider using Cachix or nix binary cache if available

---

## Open Questions / Investigation Needed

1. **PyTorch ROCm:** Does nixpkgs-unstable have a ROCm-enabled torch package?
   - Check: `nix-env -qaP 'torch*' | grep -i rocm`

2. **ROCm version compatibility:** What ROCm version does the MI50 (gfx906) require?
   - MI50 is Vega 20, should work with ROCm 5.x and 6.x

3. **xformers with ROCm:** The nixpkgs vllm depends on xformers - does it work with ROCm?

4. **Memory requirements:** 32GB VRAM should handle most models, but verify:
   - Qwen2.5-Coder-32B needs ~20GB at fp16
   - May need quantization for larger models

5. **Alternative:** If native ROCm build is too complex, the Docker approach is a viable fallback

---

## Success Criteria

1. `vllm --version` outputs version without crashing
2. `vllm serve <model>` starts successfully using MI50 GPU
3. Can query `http://localhost:8000/v1/models` and get response
4. Can use vLLM API from roo code for code completion

---

## Files to Create/Modify

| File | Action | Purpose |
|------|--------|---------|
| `vllm-service.nix` | Create | vLLM service configuration |
| `configuration.nix` | Modify | Import vllm-service.nix |
| `home.nix` | Modify | Replace `vllm` with ROCm-enabled version |

---

## References

- vLLM ROCm docs: https://docs.vllm.ai/en/latest/getting_started/amd-installation.html
- nixpkgs vllm: `pkgs/development/python-modules/vllm/default.nix`
- vLLM OpenAI server: https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html
- Your ollama config: `ollama-service.nix` (working reference)

---

## Recommended Next Steps

### Quick Validation (Docker)
```bash
# 1. Pull and test vLLM ROCm image
docker pull vllm/vllm-openai-rocm:latest

# 2. Run with small model first to verify GPU works
HIP_VISIBLE_DEVICES=GPU-0e54792172da5eeb docker run --rm -it \
  --device=/dev/kfd --device=/dev/dri \
  --group-add video \
  -p 8000:8000 --ipc=host \
  vllm/vllm-openai-rocm:latest \
  --model Qwen/Qwen2.5-Coder-1.5B-Instruct --host 0.0.0.0

# 3. Test API
curl http://localhost:8000/v1/models
```

### Native NixOS Implementation
1. Review and approve this design document
2. Choose implementation approach (global rocmSupport vs overlay)
3. Create `vllm-service.nix` configuration
4. Build and test (expect multi-hour build)
5. Configure roo code to use local API

---

## Appendix A: Docker Test Results

**Docker test on 2026-02-03 - FAILED:**
```
torch.AcceleratorError: HIP error: invalid device function
```

**Root cause: MI50 (gfx906) is NOT included in prebuilt Docker image.**

From vLLM source analysis:

| File | gfx906 Status |
|------|---------------|
| `CMakeLists.txt:40` | **Listed** in `HIP_SUPPORTED_ARCHS` |
| `Dockerfile.rocm_base:30` | **NOT included** - builds for `gfx90a;gfx942;gfx950;gfx1100;...` |
| `docs/gpu.rocm.inc.md:8` | **NOT listed** - only MI200s, MI300, MI350, RX 7900/9000 |
| `vllm/platforms/rocm.py:103` | `on_gfx9()` checks only `gfx90a`, `gfx942`, `gfx950` - **excludes gfx906** |

**Conclusion: MI50 (gfx906) is effectively unsupported by vLLM ROCm.**

Even building from source with `PYTORCH_ROCM_ARCH=gfx906`:
- Basic CUDA/HIP ops may work
- But ROCm-specific optimizations (aiter, flash attention) will NOT work because `on_gfx9()` excludes gfx906

---

## Revised Options for MI50 (gfx906)

### Option 1: Try Building vLLM from Source for gfx906 (Experimental)

Build custom Docker image:
```bash
cd ~/Downloads/vllm
DOCKER_BUILDKIT=1 docker build \
  -f docker/Dockerfile.rocm \
  --build-arg ARG_PYTORCH_ROCM_ARCH=gfx906 \
  -t vllm-rocm-gfx906 .
```

**Caveats:**
- May still fail due to missing gfx906-specific kernels
- ROCm optimizations won't be enabled
- Untested configuration

### Option 2: Stick with Ollama (Recommended)

Your `ollama-rocm` setup already works with MI50. Ollama provides:
- OpenAI-compatible API at `http://localhost:11434/v1/`
- Tested ROCm support for gfx906
- Already configured in your `ollama-service.nix`

For roo code integration, use Ollama's OpenAI-compatible endpoint instead of vLLM.

### Option 3: Use Different Inference Server

Other options that may support MI50:
- **llama.cpp** with ROCm (has gfx906 support)
- **text-generation-inference** (Hugging Face)
- **LocalAI**

### Option 4: Contribute gfx906 Support to vLLM (PR Plan)

**Files to modify for a PR:**

#### 1. `docker/Dockerfile.rocm_base` (line 30)
```dockerfile
# Before:
ARG PYTORCH_ROCM_ARCH=gfx90a;gfx942;gfx950;gfx1100;gfx1101;gfx1200;gfx1201;gfx1150;gfx1151

# After:
ARG PYTORCH_ROCM_ARCH=gfx906;gfx90a;gfx942;gfx950;gfx1100;gfx1101;gfx1200;gfx1201;gfx1150;gfx1151
```

#### 2. `vllm/platforms/rocm.py` (lines 103-105 and 133)
```python
# Line 105 - Before:
return any(arch in GPU_ARCH for arch in ["gfx90a", "gfx942", "gfx950"])

# Line 105 - After:
return any(arch in GPU_ARCH for arch in ["gfx906", "gfx90a", "gfx942", "gfx950"])

# Line 133 - Before:
ON_GFX9 = any(arch in GPU_ARCH for arch in ["gfx90a", "gfx942", "gfx950"])

# Line 133 - After:
ON_GFX9 = any(arch in GPU_ARCH for arch in ["gfx906", "gfx90a", "gfx942", "gfx950"])
```

#### 3. `docs/getting_started/installation/gpu.rocm.inc.md` (line 8)
```markdown
# Before:
- GPU: MI200s (gfx90a), MI300 (gfx942), MI350 (gfx950), ...

# After:
- GPU: MI50/MI60 (gfx906), MI200s (gfx90a), MI300 (gfx942), MI350 (gfx950), ...
```

**Note:** gfx906 is already in `CMakeLists.txt:40` `HIP_SUPPORTED_ARCHS`, so no cmake changes needed.

**PR Testing Strategy:**
1. Build Docker image locally with gfx906
2. Test basic inference on MI50
3. Verify `on_gfx9()` optimizations work or gracefully fall back

**Caveats:**
- AITER ops only support gfx942/gfx950 (won't enable on gfx906)
- MORI only supports gfx942/gfx950 (won't enable on gfx906)
- MI50 is older hardware (2018), may have lower performance than documented benchmarks

---

## Appendix B: Current NixOS vLLM Derivation Analysis

From `nix path-info --derivation`:
- Package: `python3.12-vllm-0.13.0`
- Target device: `cpu` (incorrect for your system)
- torch: `python3.12-torch-2.9.1` (CPU version)
- ROCm support: disabled

This confirms the fix requires rebuilding both torch and vllm with `rocmSupport = true`.

---

## Appendix C: Custom Docker Build Attempt (2026-02-04) - FAILED

### What We Tried

Built custom Docker images with gfx906 support:

1. **Modified `docker/Dockerfile.rocm_base`:**
   - Added `gfx906` to `PYTORCH_ROCM_ARCH`
   - Excluded `gfx906` from Flash Attention build (FA doesn't support gfx906)

2. **Modified `vllm/platforms/rocm.py`:**
   - Added `gfx906` to `on_gfx9()` function
   - Added `gfx906` to `ON_GFX9` constant

3. **Modified `docs/getting_started/installation/gpu.rocm.inc.md`:**
   - Documented MI50/MI60 (gfx906) support

4. **Built custom images:**
   - `vllm-rocm-base-gfx906` (27.2GB) - base image with PyTorch for gfx906
   - `vllm-rocm-gfx906-v2` (30.4GB) - final vLLM image

### Results

| Test | Result |
|------|--------|
| PyTorch recognizes gfx906 | ✅ `torch.cuda.get_arch_list()` includes `gfx906` |
| vLLM loads | ✅ Banner displays correctly |
| Model loading | ❌ Fails during Triton kernel compilation |

### Root Cause: ROCm Triton Doesn't Support gfx906

**Error:**
```
torch._inductor.exc.InductorError: RuntimeError: Failed to run autotuning code block: PassManager::run failed
```

**Analysis:**

The MI50/MI60 (gfx906) uses the **Vega 20** architecture (2018), which is NOT supported by ROCm Triton.

ROCm Triton only supports:
- **CDNA architecture:** gfx90a (MI200), gfx942 (MI300), gfx950 (MI350)
- **RDNA architecture:** gfx11xx (RX 7000 series), gfx12xx (RX 9000 series)

Modern vLLM heavily depends on Triton for:
- Attention kernels
- Various fused operations
- torch.compile() optimizations

Even with `--enforce-eager` mode, vLLM still uses Triton for some operations.

### Component Support Matrix for gfx906

| Component | gfx906 Support | Notes |
|-----------|----------------|-------|
| PyTorch | ✅ Can be compiled | Works with our changes |
| vLLM code | ✅ Can be compiled | Works with our changes |
| Flash Attention | ❌ No | Only supports gfx90a, gfx942, gfx950 |
| AITER | ❌ No | Only supports gfx942, gfx950 |
| MORI | ❌ No | Only supports gfx942, gfx950 |
| **ROCm Triton** | ❌ **No** | **BLOCKING - No Vega 20 support** |

### Conclusion

**MI50/MI60 (gfx906) cannot run modern vLLM** due to fundamental Triton incompatibility.

The gfx906 architecture is simply too old (2018) for the modern ROCm software stack that vLLM depends on. This is not something that can be fixed with code changes to vLLM - it would require upstream ROCm Triton to add Vega 20 support, which is unlikely given AMD's focus on newer architectures.

### Recommended Alternatives for MI50

1. **Ollama** - Already working on your system via `ollama-rocm`
2. **llama.cpp with ROCm** - Lower-level, may have better legacy support
3. **Upgrade GPU** - MI200 series (gfx90a) or newer for vLLM support

---

## Final Status

**vLLM on MI50 (gfx906): NOT POSSIBLE**

The investigation is complete. While we successfully compiled PyTorch and vLLM with gfx906 support, the runtime dependency on ROCm Triton makes vLLM unusable on MI50/MI60 GPUs.

**Continue using Ollama** for your local LLM inference needs on this system.
